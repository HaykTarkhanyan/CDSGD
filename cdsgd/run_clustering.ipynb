{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import logging\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s [%(levelname)s] %(message)s\", \n",
    "                    datefmt=\"%d-%b-%y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_z_score(data, threshold=OUTLIER_THRESHOLD_NUM_STD):\n",
    "    outliers = []\n",
    "    mean = np.mean(data)\n",
    "    std_dev = np.std(data)\n",
    "    \n",
    "    for i in data:\n",
    "        z_score = (i - mean) / std_dev \n",
    "        if np.abs(z_score) > threshold:\n",
    "            outliers.append(i)\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = os.path.join(\"..\",\"data\")\n",
    "OUTPUT_FOLDER = os.path.join(\"..\", \"data_binary_scaled_dbscan\")\n",
    "\n",
    "DATASETS = ['gaussian_df.csv', \"rectangle_df.csv\", \"uniform_df.csv\", \"wine.csv\",\n",
    "            \"breast-cancer-wisconsin.csv\"]\n",
    "\n",
    "CLUSTERING_ALG = \"dbscan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_dbscan(X_scaled, target_clusters=target_clusters, eps=eps, min_samples=min_samples, step=step, max_eps=max_eps):\n",
    "    current_clusters = 0\n",
    "    while current_clusters != target_clusters and eps <= max_eps:\n",
    "        # DBSCAN with current eps and min_samples\n",
    "        db = DBSCAN(eps=eps, min_samples=min_samples).fit(X_scaled)\n",
    "        labels = db.labels_\n",
    "        \n",
    "        # Exclude noise labels and count unique clusters\n",
    "        unique_labels = set(labels)\n",
    "        if -1 in unique_labels:\n",
    "            unique_labels.remove(-1)\n",
    "        current_clusters = len(unique_labels)\n",
    "        \n",
    "        # Check if we have the desired number of clusters\n",
    "        if current_clusters == target_clusters:\n",
    "            logging.debug(f\"Found the desired number of clusters: {current_clusters} at eps={eps}\")\n",
    "            break\n",
    "        else:\n",
    "            eps += step\n",
    "\n",
    "    # If the loop completes without breaking\n",
    "    if current_clusters != target_clusters:\n",
    "        logging.debug(\"Could not find the exact number of desired clusters with the given parameters.\")\n",
    "    else:\n",
    "        return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_adjusted_density(data, labels, radius, penalty_rate=0.5, \n",
    "                               remove_outliers=True, normalize=True):\n",
    "    \"\"\"\n",
    "    Calculate the adjusted density of each point in the dataset based on the number of points within a specified radius.\n",
    "    The density score is penalized if neighboring points belong to a different class.\n",
    "\n",
    "    Args:\n",
    "    - data (numpy array): The dataset where each row is a point in space.\n",
    "    - labels (numpy array): Class labels corresponding to each data point.\n",
    "    - radius (float): The radius of the ball within which to count neighboring points.\n",
    "    - penalty_rate (float): Penalty multiplier for points from different classes within the radius.\n",
    "    - remove_outliers (bool): Whether to remove outliers from the dataset before calculating densities.\n",
    "    \n",
    "    Returns:\n",
    "    - densities (numpy array): An array where each element is the adjusted density of the corresponding point in 'data'.\n",
    "    \"\"\"\n",
    "    tree = cKDTree(data)\n",
    "    densities = np.zeros(data.shape[0])\n",
    "    \n",
    "    for i, point in enumerate(data):\n",
    "        indices = tree.query_ball_point(point, r=radius)\n",
    "        class_counts = np.sum(labels[indices] != labels[i])\n",
    "        # Calculate density with penalty for different class points\n",
    "        densities[i] = len(indices) - 1 - (class_counts * penalty_rate)\n",
    "\n",
    "    if remove_outliers:\n",
    "        outliers = detect_outliers_z_score(densities)\n",
    "        densities = np.array([d for d in densities if d not in outliers])\n",
    "    \n",
    "    if normalize:        \n",
    "        densities = (densities - densities.min()) / (densities.max() - densities.min())\n",
    "\n",
    "    return densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-Apr-24 14:10:44 [DEBUG] ------- Processing gaussian_df.csv -------\n",
      "25-Apr-24 14:10:44 [DEBUG] Step 0: Read the data\n",
      "25-Apr-24 14:10:44 [DEBUG] Step 1: Standard scaling complete\n",
      "25-Apr-24 14:10:44 [DEBUG] Found the desired number of clusters: 2 at eps=0.22000000000000008\n",
      "25-Apr-24 14:10:44 [DEBUG] Step 2: Clustering complete\n",
      "25-Apr-24 14:10:44 [DEBUG] Step 3: Evaluation complete\n",
      "25-Apr-24 14:10:44 [DEBUG] Step 4: Dinstance calculation complete\n",
      "25-Apr-24 14:10:44 [DEBUG] Step 5: Save results complete\n",
      "25-Apr-24 14:10:44 [DEBUG] ********************\n",
      "25-Apr-24 14:10:44 [DEBUG] ------- Processing rectangle_df.csv -------\n",
      "25-Apr-24 14:10:44 [DEBUG] Step 0: Read the data\n",
      "25-Apr-24 14:10:44 [DEBUG] Step 1: Standard scaling complete\n",
      "25-Apr-24 14:10:44 [DEBUG] Found the desired number of clusters: 2 at eps=0.15000000000000002\n",
      "25-Apr-24 14:10:44 [DEBUG] Step 2: Clustering complete\n",
      "25-Apr-24 14:10:44 [DEBUG] Step 3: Evaluation complete\n",
      "25-Apr-24 14:10:44 [DEBUG] Step 4: Dinstance calculation complete\n",
      "25-Apr-24 14:10:44 [DEBUG] Step 5: Save results complete\n",
      "25-Apr-24 14:10:44 [DEBUG] ********************\n",
      "25-Apr-24 14:10:44 [DEBUG] ------- Processing uniform_df.csv -------\n",
      "25-Apr-24 14:10:44 [DEBUG] Step 0: Read the data\n",
      "25-Apr-24 14:10:44 [DEBUG] Step 1: Standard scaling complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scale': [0.5652842657018544, 0.4387547517707988], 'mean': [0.6853238581396945, 0.5424825002224375], 'var': [0.3195463010500848, 0.19250573220145528], 'silhouette': 0.5207156908434343, 'calinski_harabasz': 601.8628490066358}\n",
      "{'scale': [0.5808503703050264, 0.6307002214321649], 'mean': [-0.020608868443798903, -0.015883336495619016], 'var': [0.3373871526834863, 0.39778276931458184], 'silhouette': 0.31406335648926537, 'calinski_harabasz': 463.96284290244466}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-Apr-24 14:10:45 [DEBUG] Could not find the exact number of desired clusters with the given parameters.\n",
      "25-Apr-24 14:10:45 [WARNING] Could not find the desired number of clusters for uniform_df.csv\n",
      "25-Apr-24 14:10:45 [DEBUG] ------- Processing wine.csv -------\n",
      "25-Apr-24 14:10:45 [DEBUG] Step 0: Read the data\n",
      "25-Apr-24 14:10:45 [DEBUG] Step 1: Standard scaling complete\n"
     ]
    }
   ],
   "source": [
    "for path in DATASETS:\n",
    "    logging.debug(f\"------- Processing {path} -------\")\n",
    "    df = pd.read_csv(os.path.join(DATA_FOLDER, path))\n",
    "    \n",
    "    last_column = df.columns[-1]   \n",
    "    assert last_column == \"labels\", \"Last column is not labels\"\n",
    "     # last column is the label\n",
    "    df_not_label = df.iloc[:, :-1]\n",
    "    data_labels = df[last_column]\n",
    "    \n",
    "    assert data_labels.nunique() == 2, \"Data is not binary\"\n",
    "    logging.debug(\"Step 0: Read the data\")    \n",
    "    # perform standard scaling\n",
    "    st_scaler = StandardScaler().fit(df_not_label)\n",
    "    # break\n",
    "    df_scale = st_scaler.scale_\n",
    "    df_mean = st_scaler.mean_\n",
    "    df_var = st_scaler.var_ \n",
    "    \n",
    "    df_not_label = st_scaler.transform(df_not_label)\n",
    "\n",
    "    logging.debug(\"Step 1: Standard scaling complete\")\n",
    "\n",
    "    if CLUSTERING_ALG == \"kmeans\":\n",
    "        clustering_model = KMeans(n_clusters=2, random_state=42, n_init=\"auto\")    \n",
    "    elif CLUSTERING_ALG == \"dbscan\":\n",
    "        clustering_model = run_dbscan(df_not_label)\n",
    "        if clustering_model is None:\n",
    "            logging.warning(f\"Could not find the desired number of clusters for {path}\")\n",
    "            continue\n",
    "    \n",
    "    logging.debug(\"Step 2: Clustering complete\")\n",
    "    clustering_labels = clustering_model.fit_predict(df_not_label)\n",
    "\n",
    "    silhouette = silhouette_score(df_not_label, clustering_labels)\n",
    "    calinski_harabasz = calinski_harabasz_score(df_not_label, clustering_labels)\n",
    "    \n",
    "    if CLUSTERING_ALG == \"kmeans\":\n",
    "        # print(f\"Inertia for {path} is {inertia}\")\n",
    "        inertia = clustering_model.inertia_\n",
    "    # print(f\"Silhouette score for {path} is {silhouette}\")\n",
    "    # print(f\"Calinski Harabasz score for {path} is {calinski_harabasz}\")\n",
    "    # print()\n",
    "    \n",
    "    df.iloc[:, :-1] = df_not_label\n",
    "    df[\"labels_clustering\"] = clustering_labels\n",
    "    \n",
    "    \n",
    "    logging.debug(\"Step 3: Evaluation complete\")\n",
    "    \n",
    "    # if df.shape[1] == 3:\n",
    "    #     print(path)\n",
    "    #     fig = px.scatter(df, x=df.columns[0], y=df.columns[1], color=\"labels_clustering\")\n",
    "    #     fig.show()\n",
    "    #     fig = px.scatter(df, x=df.columns[0], y=df.columns[1], color=\"labels\")\n",
    "    #     fig.show()\n",
    "    \n",
    "    # calculate each points distance to the centroids\n",
    "\n",
    "    if CLUSTERING_ALG == \"kmeans\":\n",
    "        df[\"distance\"] = np.min(\n",
    "            np.linalg.norm(df_not_label[:, np.newaxis] - clustering_model.cluster_centers_, axis=2), axis=1)\n",
    "    else:\n",
    "        df[\"distance\"] = calculate_adjusted_density(df_not_label, clustering_model.labels_, \n",
    "                                                    radius=0.5, penalty_rate=0.5, remove_outliers=False, normalize=False)\n",
    "\n",
    "    # detect outliers\n",
    "    outliers = detect_outliers_z_score(df[\"distance\"])\n",
    "\n",
    "    df[\"outlier\"] = df[\"distance\"].apply(lambda x: x in outliers)\n",
    "\n",
    "    df_okay = df[~df[\"outlier\"]]\n",
    "\n",
    "    # normalize distance to centroid for each cluster\n",
    "    df[\"distance_norm\"] = df_okay.groupby(\"labels_clustering\")[\"distance\"].transform(\n",
    "        lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "    \n",
    "    logging.debug(\"Step 4: Dinstance calculation complete\")\n",
    "    # make \"labels_clustering\" be the last column\n",
    "    cols_to_drop = [\"distance\", \"outlier\"]\n",
    "    \n",
    "    df = df[[col for col in df.columns if col not in [\"labels_clustering\"] + cols_to_drop] + [\"labels_clustering\"]]\n",
    "    \n",
    "    \n",
    "    res = {\"scale\": list(df_scale), \"mean\": list(df_mean), \"var\": list(df_var), \"silhouette\": silhouette, \n",
    "            \"calinski_harabasz\": calinski_harabasz}\n",
    "    if CLUSTERING_ALG == \"kmeans\":\n",
    "        res[\"inertia\"] = inertia\n",
    "    print(res)\n",
    "    \n",
    "    PATH = os.path.join(OUTPUT_FOLDER, \"results\")\n",
    "    with open(os.path.join(PATH, f\"{CLUSTERING_ALG}_{path.split('.')[0]}.json\"), \"w\") as f:\n",
    "        json.dump(res, f, indent=4)\n",
    "    \n",
    "    df.to_csv(os.path.join(OUTPUT_FOLDER, f\"kmeans_{path}\"), index=False)\n",
    "    logging.debug(\"Step 5: Save results complete\")\n",
    "    # logging.debug(\"*\"*20)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 16\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Generating sample data\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# X, _ = make_moons(n_samples=300, noise=0.1, random_state=42)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# X = df[[df.columns[0], df.columns[1]]].values\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# X_scaled = StandardScaler().fit_transform(X)  # Scaling is important for DBSCAN\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_dbscan\u001b[39m(X_scaled, target_clusters\u001b[38;5;241m=\u001b[39m\u001b[43mtarget_clusters\u001b[49m, eps\u001b[38;5;241m=\u001b[39meps, min_samples\u001b[38;5;241m=\u001b[39mmin_samples, step\u001b[38;5;241m=\u001b[39mstep, max_eps\u001b[38;5;241m=\u001b[39mmax_eps):\n\u001b[0;32m     18\u001b[0m     current_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m current_clusters \u001b[38;5;241m!=\u001b[39m target_clusters \u001b[38;5;129;01mand\u001b[39;00m eps \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_eps:\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;66;03m# DBSCAN with current eps and min_samples\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'target_clusters' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generating sample data\n",
    "# X, _ = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "\n",
    "# path = os.path.join(OUTPUT_FOLDER, \"kmeans_gaussian_df.csv\")\t\n",
    "# df = pd.read_csv(path)\n",
    "\n",
    "# X = df[[df.columns[0], df.columns[1]]].values\n",
    "# X_scaled = StandardScaler().fit_transform(X)  # Scaling is important for DBSCAN\n",
    "\n",
    "\n",
    "def run_dbscan(X_scaled, target_clusters=target_clusters, eps=eps, min_samples=min_samples, step=step, max_eps=max_eps):\n",
    "    \n",
    "    current_clusters = 0\n",
    "    while current_clusters != target_clusters and eps <= max_eps:\n",
    "        # DBSCAN with current eps and min_samples\n",
    "        db = DBSCAN(eps=eps, min_samples=min_samples).fit(X_scaled)\n",
    "        labels = db.labels_\n",
    "        \n",
    "        # Exclude noise labels and count unique clusters\n",
    "        unique_labels = set(labels)\n",
    "        if -1 in unique_labels:\n",
    "            unique_labels.remove(-1)\n",
    "        current_clusters = len(unique_labels)\n",
    "        \n",
    "        # Check if we have the desired number of clusters\n",
    "        if current_clusters == target_clusters:\n",
    "            print(f\"Found the desired number of clusters: {current_clusters} at eps={eps}\")\n",
    "            break\n",
    "        else:\n",
    "            eps += step\n",
    "\n",
    "    # If the loop completes without breaking\n",
    "    if current_clusters != target_clusters:\n",
    "        print(\"Could not find the exact number of desired clusters with the given parameters.\")\n",
    "    else:\n",
    "        return db\n",
    "# else:\n",
    "#     # Further analysis or visualization can go here\n",
    "#     import matplotlib.pyplot as plt\n",
    "    \n",
    "#     # Plotting results\n",
    "#     core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "#     core_samples_mask[db.core_sample_indices_] = True\n",
    "#     for k in unique_labels:\n",
    "#         class_member_mask = (labels == k)\n",
    "        \n",
    "#         xy = X_scaled[class_member_mask & core_samples_mask]\n",
    "#         plt.plot(xy[:, 0], xy[:, 1], 'o', markeredgecolor='k')\n",
    "        \n",
    "#         xy = X_scaled[class_member_mask & ~core_samples_mask]\n",
    "#         plt.plot(xy[:, 0], xy[:, 1], 'o', markeredgecolor='k')\n",
    "    \n",
    "#     plt.title('DBSCAN clustering with 2 clusters')\n",
    "#     plt.show()\n",
    "\n",
    "#     # save the results\n",
    "#     df[\"labels_dbscan\"] = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Example data generation\n",
    "# X, y = make_blobs(n_samples=200, centers=3, random_state=42, cluster_std=5)\n",
    "\n",
    "# X = df[[df.columns[0], df.columns[1]]].values\n",
    "# y = df[\"labels_dbscan\"].values\n",
    "\n",
    "# # Radius within which to count neighbors\n",
    "# radius = 0.5\n",
    "\n",
    "# # Calculate adjusted densities\n",
    "# adjusted_densities = calculate_adjusted_density(X, y, radius, penalty_rate=0.5)\n",
    "\n",
    "# df[\"distance\"] = adjusted_densities\n",
    "\n",
    "# df[\"distance_norm\"] = (df[\"distance\"] - df[\"distance\"].min()) / (df[\"distance\"].max() - df[\"distance\"].min()) \n",
    "\n",
    "\n",
    "# point_min = df[df[\"distance_norm\"] == df[\"distance_norm\"].min()]\n",
    "# point_max = df[df[\"distance_norm\"] == df[\"distance_norm\"].max()]\n",
    "\n",
    "# # Plotting\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# scatter = plt.scatter(X[:, 0], X[:, 1], c=adjusted_densities, cmap='viridis', edgecolor='k', s=50)\n",
    "# plt.colorbar(scatter, label='Adjusted Point Density')\n",
    "# plt.title('Adjusted Point Densities with a Penalty Rate')\n",
    "# plt.xlabel('Feature 1')\n",
    "# plt.ylabel('Feature 2')\n",
    "\n",
    "# plt.scatter(point_min[point_min.columns[0]], point_min[point_min.columns[1]], c='red', s=200, marker='x', label='Min')\n",
    "# plt.scatter(point_max[point_max.columns[0]], point_max[point_max.columns[1]], c='blue', s=200, marker='x', label='Max')\n",
    "\n",
    "# # plot circle if radius 2 around the point\n",
    "# circle_min = plt.Circle((point_min[point_min.columns[0]], point_min[point_min.columns[1]]), radius, color='red', fill=False)\n",
    "# circle_max = plt.Circle((point_max[point_max.columns[0]], point_max[point_max.columns[1]]), radius, color='blue', fill=False)\n",
    "\n",
    "# plt.gca().add_artist(circle_min)\n",
    "# plt.gca().add_artist(circle_max)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "495    1\n",
       "496    1\n",
       "497    1\n",
       "498    1\n",
       "499    1\n",
       "Name: labels_dbscan, Length: 500, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"labels_dbscan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"distance\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
